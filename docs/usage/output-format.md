# Output Format

This page describes the format and meaning of all BactScout output files.

## CSV Output Files

### `final_summary.csv` (Batch Summary)

Generated by the `summary` command, contains one row per sample with consolidated results.

| Column | Type | Description |
|--------|------|-------------|
| `sample_id` | String | Sample identifier extracted from filename |
| `species` | String | Best-matching species from Sylph GTDB matching |
| `coverage` | Float | Mean coverage depth (reads per base) |
| `q30_percent` | Float | Percentage of bases with Phred quality ≥30 |
| `mean_read_length` | Float | Average length of sequencing reads (bp) |
| `total_bases` | Integer | Total number of sequenced bases |
| `gc_content` | Float | GC content percentage of the sample |
| `reference_size_mbp` | Float | Reference genome size (Mb) |
| `quality_pass` | String | PASS or FAIL based on QC thresholds |
| `contamination_pct` | Float | Percentage of reads from other species |
| `mlst_type` | String | MLST sequence type (if available) |
| `sequencing_platform` | String | Detected sequencing platform |

### `sample_results.csv` (Per-Sample Results)

Generated by `qc` or `collect` for each sample, contains detailed metrics.

| Column | Type | Description |
|--------|------|-------------|
| `sample_id` | String | Sample identifier |
| `species` | String | Primary species identification |
| `coverage` | Float | Mean sequencing depth (x) |
| `q30_percent` | Float | Bases with Q≥30 (%) |
| `mean_read_length` | Float | Average read length (bp) |
| `total_bases` | Integer | Total sequenced bases |
| `gc_content` | Float | GC content (%) |
| `reference_genome_size` | Float | Reference size (Mb) |
| `quality_status` | String | PASS or FAIL |
| `primary_species` | String | Best match species |
| `primary_species_pct` | Float | % identity to primary species |
| `secondary_species` | String | Second-best match (if applicable) |
| `secondary_species_pct` | Float | % identity to secondary species |
| `contamination_percent` | Float | Total contamination (%) |
| `est_contamination_sources` | Integer | Number of contaminating species detected |
| `resource_threads_peak` | Integer | Peak number of threads used (if `--report-resources` enabled) |
| `resource_memory_peak_mb` | Float | Peak memory usage in MB (if `--report-resources` enabled) |
| `resource_memory_avg_mb` | Float | Average memory usage in MB (if `--report-resources` enabled) |
| `resource_duration_sec` | Float | Total analysis duration in seconds (if `--report-resources` enabled) |

## JSON Output Files

### `fastp_report.json`

Raw quality metrics from fastp:

```json
{
  "summary": {
    "before_filtering": {
      "total_reads": 1000000,
      "total_bases": 150000000,
      "mean_length": 150,
      "quality": 35.2
    },
    "after_filtering": {
      "total_reads": 980000,
      "total_bases": 147000000,
      "mean_length": 150,
      "quality": 36.1
    }
  },
  "adapter_trimming": {
    "reads_with_adapters": 15000,
    "bases_trimmed": 45000
  }
}
```

### `reads_QC.json`

Processed quality control metrics:

```json
{
  "total_reads": 1000000,
  "total_bases": 150000000,
  "mean_read_length": 150.0,
  "median_read_length": 151,
  "min_read_length": 50,
  "max_read_length": 151,
  "q30_bases_pct": 92.5,
  "gc_content_pct": 52.3,
  "median_quality": 37
}
```

### `stringmlst_results.json`

MLST typing results:

```json
{
  "species": "Escherichia coli",
  "st": "ST-10",
  "alleles": {
    "adk": 1,
    "fumC": 18,
    "gyrB": 2,
    "icd": 3,
    "mdh": 1,
    "purA": 1,
    "recA": 5
  },
  "status": "complete"
}
```

## TSV Output Files

### `sylph_matches.tsv`

Taxonomic profiling from Sylph:

| Column | Description |
|--------|-------------|
| `query_id` | Sample identifier |
| `query_len` | Total bases in sample |
| `ref_id` | Reference database ID |
| `ref_name` | Species name |
| `ani` | Average nucleotide identity (%) |
| `match_percent` | Percentage of sample matching reference |
| `num_matches` | Number of k-mer matches |

Rows are sorted by ANI in descending order (best match first).

## HTML Output Files

### `fastp_report.html`

Interactive quality control report with:
- Per-base quality scores (before/after filtering)
- Per-read quality distribution
- Read length distribution
- GC content histogram
- Adapter detection and trimming info
- Duplication analysis (if enabled)

Open in any web browser for detailed visualization.

## Directory Structure

```
bactscout_output/
├── final_summary.csv                    # All results (summary command)
├── Sample_001/
│   ├── fastp_report.html               # Interactive QC report
│   ├── fastp_report.json               # Raw fastp data
│   ├── reads_QC.json                   # Processed QC metrics
│   ├── sample_results.csv              # Per-sample results
│   ├── sylph_matches.tsv               # Taxonomic matches
│   ├── sylph_matches.csv               # Taxonomic matches (CSV)
│   ├── stringmlst_results.json         # MLST typing
│   ├── stringmlst_report.txt           # MLST summary
│   └── [intermediate files]
├── Sample_002/
│   ├── fastp_report.html
│   ├── sample_results.csv
│   └── ...
└── ...
```

## Quality Thresholds

The `quality_pass` column determination uses these defaults (from config):

| Metric | Threshold | Column |
|--------|-----------|--------|
| Coverage | ≥ 30x | `coverage` |
| Q30 Bases | ≥ 80% | `q30_percent` |
| Read Length | ≥ 100 bp | `mean_read_length` |
| Contamination | ≤ 10% | `contamination_pct` |

A sample **PASS**es only if **all** metrics meet thresholds.

Modify thresholds in `bactscout_config.yml`:

```yaml
coverage_threshold: 30
q30_pass_threshold: 0.80
read_length_pass_threshold: 100
contamination_threshold: 10
```

## Resource Usage Reporting

When the `--report-resources` flag is used with `qc` or `collect` commands, BactScout tracks system resource usage during analysis and includes the following columns in output CSV files:

| Column | Unit | Description |
|--------|------|-------------|
| `resource_threads_peak` | count | Maximum number of threads used during sample processing |
| `resource_memory_peak_mb` | MB | Peak memory consumption during analysis |
| `resource_memory_avg_mb` | MB | Average memory consumption during analysis |
| `resource_duration_sec` | seconds | Total time spent analyzing the sample |

### Enabling Resource Reporting

```bash
# Track resources for batch QC
pixi run bactscout qc /path/to/samples/ --report-resources

# Track resources for single sample
pixi run bactscout collect sample_R1.fastq.gz sample_R2.fastq.gz --report-resources
```

### Example Analysis

```python
import pandas as pd

# Load results with resource tracking
df = pd.read_csv('bactscout_output/final_summary.csv')

# Find memory-intensive samples
high_memory = df[df['resource_memory_peak_mb'] > 2000]
print(f"Samples using >2GB memory: {len(high_memory)}")

# Analyze processing time distribution
print(f"Average processing time: {df['resource_duration_sec'].mean():.1f}s")
print(f"Slowest sample: {df['resource_duration_sec'].max():.1f}s")

# Correlate resources with sample size
df['bases_per_second'] = df['total_bases'] / df['resource_duration_sec']
print(f"Average throughput: {df['bases_per_second'].mean():.0f} bp/s")
```

## Reading Results Programmatically

### Python - Pandas

```python
import pandas as pd

# Load summary
df = pd.read_csv('bactscout_output/final_summary.csv')

# Quick statistics
print(df.describe())

# Filter results
high_qual = df[df['quality_pass'] == 'PASS']
print(f"{len(high_qual)} / {len(df)} samples passed QC")

# Group by species
for species in df['species'].unique():
    count = len(df[df['species'] == species])
    print(f"{species}: {count} samples")
```

### Python - JSON

```python
import json

# Load MLST results
with open('bactscout_output/Sample_001/stringmlst_results.json') as f:
    mlst = json.load(f)
    
print(f"ST: {mlst['st']}")
print(f"Species: {mlst['species']}")

# Load QC metrics
with open('bactscout_output/Sample_001/reads_QC.json') as f:
    qc = json.load(f)
    
print(f"Coverage: {qc['total_bases'] / 1e6 / 5:.0f}x")  # Assuming 5 Mb ref
print(f"Q30: {qc['q30_bases_pct']:.1f}%")
```

### R - Reading CSV

```r
library(tidyverse)

# Load summary
results <- read_csv('bactscout_output/final_summary.csv')

# Quick stats
results %>%
  summarize(
    total_samples = n(),
    pass_rate = sum(quality_pass == 'PASS') / n() * 100,
    mean_coverage = mean(coverage)
  )

# Species distribution
results %>%
  group_by(species) %>%
  summarize(count = n(), mean_cov = mean(coverage))
```

## Common Analysis Queries

### Find Failed Samples

```python
failed = df[df['quality_pass'] == 'FAIL']
print(f"Failed samples: {len(failed)}")
print(failed[['sample_id', 'quality_pass', 'coverage', 'q30_percent']])
```

### Identify Contamination

```python
contaminated = df[df['contamination_pct'] > 5]
print(f"Contaminated (>5%): {len(contaminated)}")
```

### Species-Specific QC

```python
ecoli = df[df['species'] == 'Escherichia coli']
print(f"E. coli samples: {len(ecoli)}")
print(f"Avg coverage: {ecoli['coverage'].mean():.1f}x")
print(f"Pass rate: {(ecoli['quality_pass'] == 'PASS').sum() / len(ecoli) * 100:.1f}%")
```

See [Quality Control Guide](../guide/quality-control.md) for interpretation guidelines.
